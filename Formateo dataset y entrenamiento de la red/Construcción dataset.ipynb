{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d592432-5818-41c3-b3f4-4d8fe553d1e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Construcción dataset\n",
    "\n",
    "En este apartado se pretende realizar la lectura de los ficheros y unificarlos en uno solo para el posterior entreno de la red, ya que ambos recogen datos de las mediciones de los sensores (acelerómetro, giroscopio y magnetómetro) por separado.\n",
    "\n",
    "El dataset generado tendrá la información de todos los sensores formando un vector de 10 dimensiones, 9 dimensiones de entrada por los sensores más la última correspondiente a la salida.\n",
    "\n",
    "En caso de que posteriormente se desee probar la red con menos entradas sería cuestión de saltar las columnas pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1186183-16a8-4da1-895e-91236529f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "datasetType = [\"train\",\"test\"]\n",
    "classification = {\"parado\":0,\"caminar\":1,\"correr\":2,\"bici\":3}\n",
    "folder = \"./data\"\n",
    "raw = \"./rawdata\"\n",
    "headers = [\"X(mg)\",\"Y(mg)\",\"Z(mg)\",\"X(dps)\",\"Y(dps)\",\"Z(dps)\",\"X(mGa)\",\"Y(mGa)\",\"Z(mGa)\",\"Output\"] #Acelerómetro,giroscopio y magnetómetro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5ce46-104f-430c-be94-753dd387d8b4",
   "metadata": {},
   "source": [
    "+ ### Elección de parámetros\n",
    "Para automatizar el proceso de la creación del fichero de entrenamiento, hay que identificar los parámetros necesarios para el correcto funcionamiento:\n",
    "\n",
    "    + **Número de sensores.**\n",
    "    + **Dato a clasificar.**\n",
    "    + **Tipo de dataset (entrenamiento o validación).**\n",
    "Este último parámetro lo establece el usuario indicandolo por teclado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7e5cee-596e-40fb-a5a4-65f94bcc46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParams():\n",
    "    try:\n",
    "        for e in classification.keys():\n",
    "            aux = \"./\" + e\n",
    "            if not os.path.exists(aux):\n",
    "                os.makedirs(aux)\n",
    "            \n",
    "        dataType = int(input(\"Especifique si los datos forman parte del conjunto de entrenamiento o validación: \\n 0 --> Entrenamiento \\n 1 --> Validación\\n\")) \n",
    "        if dataType > 1 or dataType < 0:\n",
    "            raise Exception (\"Valor entre 0 y 1, repita de nuevo\")\n",
    "        \n",
    "        resultFolder = \"./\" + datasetType[dataType]\n",
    "        if not os.path.exists(resultFolder):\n",
    "            os.makedirs(resultFolder)\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Formato incorrecto\")\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "    if not os.path.exists(raw):\n",
    "        os.makedirs(raw)\n",
    "    return dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a05bf8-8275-4c2e-8648-a3e438c4ad3f",
   "metadata": {},
   "source": [
    " + ### Lectura de ficheros ###\n",
    " Para extraer las rutas de los ficheros creamos un diccionario en el que las claves son el tipo de clasificación al que pertenecen los datos y los valores las rutas de los ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854cdf75-08bc-443e-b0ac-3fa0bf44811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRoutes():\n",
    "    files = []\n",
    "    rutas = [e for e in os.listdir() if e in classification.keys()]\n",
    "    for e in rutas:\n",
    "        for r in os.listdir(e):\n",
    "            if re.search(\"\\.csv\",r):\n",
    "                files.append(\"./\" + e + \"/\" + r)\n",
    "    files.sort()\n",
    "    dicc = {i:[] for i in rutas}\n",
    "    for r in rutas:\n",
    "        for f in files:\n",
    "            if r in f:\n",
    "                dicc[r].append(f)\n",
    "    return dicc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207084c-ac0f-4778-a486-f03df2bfcd25",
   "metadata": {},
   "source": [
    "+ **A continuación almacenamos en una lista de listas todos los datos de los ficheros.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea3b9b7-0737-4b22-88db-a27871c6f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFiles(files):\n",
    "    dataFiles = []\n",
    "    for f in files:\n",
    "        aux = list()\n",
    "        with open(f,'r') as file:\n",
    "            lector = csv.reader(file,delimiter = ',')\n",
    "            for i in range(4):\n",
    "                next(lector)\n",
    "            for e in lector:\n",
    "                e = e[-4:-1]\n",
    "                aux.append(e)\n",
    "        file.close()\n",
    "        dataFiles.append(aux)\n",
    "    return dataFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26451512-bfbe-40c2-bfba-41d640d41ecd",
   "metadata": {},
   "source": [
    "+ **Ya solo queda formar un vector con los datos de los sensores alineados para introducirlo como entrada posteriormente a la red.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a057e878-fb02-4f17-a341-2bc9718f428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifyFiles(dataFiles,output):\n",
    "    unifiedData = []\n",
    "    for z in range (0,len(dataFiles),3):\n",
    "        for i in range (0,len(dataFiles[z])):\n",
    "            aux = []\n",
    "            for j in range(z,z+3):\n",
    "                aux.extend(dataFiles[j][i])\n",
    "            aux.append(output)\n",
    "            unifiedData.append(aux)\n",
    "    return unifiedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2353ea3-6286-4827-ab3e-d3e191a5ed65",
   "metadata": {},
   "source": [
    "+ **Con el vector de entrada alineado lo guardamos en un fichero.**\n",
    "    \n",
    "    Antes de completar el dataset almacenamos en ficheros por separado cada categoría a clasificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392652c-8a4d-441f-938d-00225863bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFilescategory(unifiedData,output,fileName):\n",
    "    ruta = folder + \"/\" + fileName + \"/{}.csv\".format(output)\n",
    "    file = open(ruta, 'w',newline ='')\n",
    "    #random.shuffle(unifiedData) #Comentar para comprobar que se guardan bien los datos\n",
    "    with file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(unifiedData)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c45aa4-4d4c-4333-85a3-88b46594b3e1",
   "metadata": {},
   "source": [
    "+ **Por último generamos el dataset que emplearemos para entrenar a la red.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcf6a1d-d5a4-4e8c-8326-e880d764427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDataset(dataType,fileName):\n",
    "    dataFiles = []\n",
    "    directory = folder + \"/\" + fileName\n",
    "    for e in os.listdir(directory):\n",
    "         if re.search(\"\\.csv\",e):\n",
    "            dataFiles.append(e)\n",
    "   \n",
    "    finalDataset = datasetType[dataType] + \"/\" + fileName + \".csv\"\n",
    "    aux = []\n",
    "    for e in dataFiles:\n",
    "        e = directory + \"/\" + e\n",
    "        with open(e,'r') as t:\n",
    "            reader = csv.reader(t,delimiter = ',')\n",
    "            next(reader)\n",
    "            for r in reader:\n",
    "                aux.append(r)\n",
    "        t.close()\n",
    "    \n",
    "\n",
    "    with open(finalDataset,'w',newline = '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(aux)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1559de-13d3-4678-90ee-146b258e016a",
   "metadata": {},
   "source": [
    "+ **Por último para completar el dataset solo quedaría llamar de forma ordenada las funciones definidas anteriormente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06563ae-ba65-4805-8c46-b391d3bf5ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Especifique si los datos forman parte del conjunto de entrenamiento o validación: \n",
      " 0 --> Entrenamiento \n",
      " 1 --> Validación\n",
      " 1\n",
      "Introduzca el nombre del fichero para guardar el dataset:  tfgtest\n"
     ]
    }
   ],
   "source": [
    "dataType = initParams()\n",
    "fileName = input(\"Introduzca el nombre del fichero para guardar el dataset: \") \n",
    "\n",
    "datafolder = folder + \"/\" + fileName\n",
    "if not os.path.exists(datafolder):\n",
    "    os.makedirs(datafolder)\n",
    "\n",
    "routes = getRoutes()\n",
    "for k,v in routes.items():\n",
    "    files = v\n",
    "    output = classification[k]\n",
    "    dataFiles = readFiles(files)\n",
    "    uniFiedData = unifyFiles(dataFiles,output)\n",
    "    writeFilescategory(uniFiedData,k,fileName)\n",
    "\n",
    "completeDataset(dataType,fileName) #Realmente este fichero no se usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2265becd-e6c9-450a-b6e0-cf909ab89a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [item for lista in routes.values() for item in lista]\n",
    "moveFolder = raw + \"/\" + fileName\n",
    "if not os.path.exists(moveFolder):\n",
    "    os.makedirs(moveFolder)\n",
    "\n",
    "for k,v in routes.items():\n",
    "    r = moveFolder + \"/\" + k\n",
    "    if not os.path.exists(r):\n",
    "        os.makedirs(r)\n",
    "    for e in v:\n",
    "        shutil.move(e,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d544e-07b6-41bb-96c1-465b9501ec3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
